{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cagrisar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Instagram Influencer Classification and Regression Task...\n",
      "Loading data...\n",
      "\n",
      "Base training data distribution:\n",
      "category\n",
      "food                    511\n",
      "health and lifestyle    503\n",
      "tech                    346\n",
      "entertainment           323\n",
      "fashion                 299\n",
      "travel                  294\n",
      "art                     191\n",
      "mom and children        149\n",
      "sports                  113\n",
      "gaming                   13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Annotated data distribution:\n",
      "category\n",
      "entertainment           27\n",
      "food                    23\n",
      "health and lifestyle    23\n",
      "art                     15\n",
      "sports                  14\n",
      "fashion                 14\n",
      "tech                    13\n",
      "travel                   7\n",
      "gaming                   7\n",
      "mom and children         5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Combined distribution:\n",
      "food                    520\n",
      "health and lifestyle    514\n",
      "tech                    353\n",
      "entertainment           333\n",
      "fashion                 303\n",
      "travel                  294\n",
      "art                     194\n",
      "mom and children        149\n",
      "sports                  124\n",
      "gaming                   18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of labeled profiles: 2804\n",
      "\n",
      "Matched 2775 profiles with posts and profile data\n",
      "\n",
      "Final training data distribution:\n",
      "food                    516\n",
      "health and lifestyle    509\n",
      "tech                    350\n",
      "entertainment           328\n",
      "fashion                 302\n",
      "travel                  293\n",
      "art                     193\n",
      "mom and children        149\n",
      "sports                  121\n",
      "gaming                   14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test data loaded: 999 profiles\n",
      "\n",
      "Preparing classification training data...\n",
      "\n",
      "=== TRAINING CLASSIFICATION MODEL WITH SMOTE ===\n",
      "\n",
      "Original class distribution:\n",
      "art: 154\n",
      "entertainment: 262\n",
      "fashion: 242\n",
      "food: 413\n",
      "gaming: 11\n",
      "health and lifestyle: 407\n",
      "mom and children: 119\n",
      "sports: 97\n",
      "tech: 280\n",
      "travel: 235\n",
      "\n",
      "Class distribution after SMOTE:\n",
      "art: 413\n",
      "entertainment: 413\n",
      "fashion: 413\n",
      "food: 413\n",
      "gaming: 413\n",
      "health and lifestyle: 413\n",
      "mom and children: 413\n",
      "sports: 413\n",
      "tech: 413\n",
      "travel: 413\n",
      "\n",
      "Performing grid search with SMOTE-balanced data...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagrisar/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found:\n",
      "{'bootstrap': False, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "Best cross-validation balanced accuracy score: 0.8914\n",
      "\n",
      "Top 10 most important features for classification:\n",
      "                 feature  importance\n",
      "2729              lezzet    0.008850\n",
      "0         follower_count    0.005116\n",
      "11             std_likes    0.004451\n",
      "14       avg_emoji_count    0.004196\n",
      "13    avg_caption_length    0.003959\n",
      "699                bebek    0.003951\n",
      "2739            lezzetli    0.003917\n",
      "10             avg_likes    0.003857\n",
      "2960              mobile    0.003797\n",
      "2732             lezzeti    0.003746\n",
      "\n",
      "Preparing regression data...\n",
      "\n",
      "=== PREPARING REGRESSION DATA ===\n",
      "Regression dataset shape: X=(92260, 9), y=(92260,)\n",
      "\n",
      "=== TRAINING REGRESSION MODEL ===\n",
      "\n",
      "Performing grid search...\n",
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagrisar/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found:\n",
      "{'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "\n",
      "Best cross-validation R² score: 0.9167\n",
      "\n",
      "Top features importance for regression:\n",
      "                  feature  importance\n",
      "8          comments_count    0.394470\n",
      "0          follower_count    0.322411\n",
      "3             is_verified    0.086875\n",
      "1         following_count    0.066567\n",
      "5          caption_length    0.045201\n",
      "2              post_count    0.042829\n",
      "4             is_business    0.030433\n",
      "7  media_type_is_carousel    0.006879\n",
      "6     media_type_is_video    0.004335\n",
      "\n",
      "=== MODEL EVALUATION ===\n",
      "\n",
      "Classification Accuracy: 0.6180\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "                 art       0.38      0.13      0.19        39\n",
      "       entertainment       0.37      0.38      0.37        66\n",
      "             fashion       0.58      0.65      0.61        60\n",
      "                food       0.79      0.93      0.85       103\n",
      "              gaming       0.00      0.00      0.00         3\n",
      "health and lifestyle       0.54      0.60      0.56       102\n",
      "    mom and children       0.88      0.47      0.61        30\n",
      "              sports       0.88      0.62      0.73        24\n",
      "                tech       0.58      0.77      0.66        70\n",
      "              travel       0.76      0.59      0.66        58\n",
      "\n",
      "            accuracy                           0.62       555\n",
      "           macro avg       0.57      0.51      0.53       555\n",
      "        weighted avg       0.62      0.62      0.60       555\n",
      "\n",
      "\n",
      "Regression Metrics:\n",
      "MSE (log10): 0.0803\n",
      "MAE (log10): 0.2070\n",
      "R² Score: 0.9227\n",
      "\n",
      "=== GENERATING PREDICTIONS ===\n",
      "\n",
      "Generating classification predictions...\n",
      "\n",
      "Generating regression predictions...\n",
      "\n",
      "Saving predictions to prediction-classification-round1.json\n",
      "Successfully saved predictions to prediction-classification-round1.json\n",
      "\n",
      "Saving predictions to prediction-regression-round1.json\n",
      "Successfully saved predictions to prediction-regression-round1.json\n",
      "\n",
      "Task completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "nltk.download('stopwords')\n",
    "turkish_stopwords = stopwords.words('turkish')\n",
    "\n",
    "class InfluencerFeatureExtractor:\n",
    "    def __init__(self, max_features=5000):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words=turkish_stopwords,\n",
    "            max_features=max_features,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        self.scaler = RobustScaler()\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> tuple:\n",
    "        \"\"\"Enhanced text preprocessing\"\"\"\n",
    "        if text is None or not isinstance(text, str):\n",
    "            return \"\", 0\n",
    "            \n",
    "        # lowercase convert\n",
    "        text = text.casefold()\n",
    "        \n",
    "        emoji_count = sum(1 for c in text if ord(c) > 127)\n",
    "        \n",
    "        # remove urls\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # keep Turkish characters but remove other specials\n",
    "        text = re.sub(r'[^a-zçğıöşü0-9\\s#@]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text, emoji_count\n",
    "\n",
    "    def extract_profile_features(self, profile):\n",
    "        \"\"\"Extract numerical and categorical features from profile\"\"\"\n",
    "        try:\n",
    "            def safe_float(value):\n",
    "                if value is None:\n",
    "                    return 0.0\n",
    "                try:\n",
    "                    if isinstance(value, str):\n",
    "                        value = value.replace(',', '')\n",
    "                    return float(value)\n",
    "                except (ValueError, TypeError):\n",
    "                    return 0.0\n",
    "\n",
    "            features = {\n",
    "                'follower_count': safe_float(profile.get('follower_count')),\n",
    "                'following_count': safe_float(profile.get('following_count')),\n",
    "                'post_count': safe_float(profile.get('post_count')),\n",
    "                'is_business': float(bool(profile.get('is_business_account'))),\n",
    "                'is_private': float(bool(profile.get('is_private'))),\n",
    "                'is_verified': float(bool(profile.get('is_verified'))),\n",
    "                'has_website': float(bool(profile.get('external_url'))),\n",
    "                'has_business_email': float(bool(profile.get('business_email'))),\n",
    "                'bio_length': float(len(str(profile.get('biography', ''))))\n",
    "            }\n",
    "            \n",
    "            # engagement ratios\n",
    "            follower_count = features['follower_count']\n",
    "            features['following_ratio'] = (\n",
    "                features['following_count'] / follower_count if follower_count > 0 else 0.0\n",
    "            )\n",
    "            \n",
    "            return features\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing profile {profile.get('username', 'unknown')}: {str(e)}\")\n",
    "            return {\n",
    "                'follower_count': 0.0,\n",
    "                'following_count': 0.0,\n",
    "                'post_count': 0.0,\n",
    "                'is_business': 0.0,\n",
    "                'is_private': 0.0,\n",
    "                'is_verified': 0.0,\n",
    "                'has_website': 0.0,\n",
    "                'has_business_email': 0.0,\n",
    "                'bio_length': 0.0,\n",
    "                'following_ratio': 0.0\n",
    "            }\n",
    "\n",
    "    def extract_post_features(self, posts):\n",
    "        \"\"\"Extract features from user posts\"\"\"\n",
    "        if not posts:\n",
    "            return {\n",
    "                'avg_likes': 0,\n",
    "                'std_likes': 0,\n",
    "                'avg_comments': 0,\n",
    "                'avg_caption_length': 0,\n",
    "                'avg_emoji_count': 0,\n",
    "                'avg_hashtags': 0,\n",
    "                'avg_mentions': 0\n",
    "            }\n",
    "            \n",
    "        likes = []\n",
    "        comments = []\n",
    "        caption_lengths = []\n",
    "        emoji_counts = []\n",
    "        hashtag_counts = []\n",
    "        mention_counts = []\n",
    "        \n",
    "        for post in posts:\n",
    "            # Engagement metrics\n",
    "            like_count = post.get('like_count')\n",
    "            if like_count is not None:\n",
    "                likes.append(float(like_count))\n",
    "                \n",
    "            comment_count = post.get('comments_count')\n",
    "            if comment_count is not None:\n",
    "                comments.append(float(comment_count))\n",
    "            \n",
    "            # Caption analysis\n",
    "            caption = post.get('caption', '')\n",
    "            if caption:\n",
    "                caption_text, emoji_count = self.preprocess_text(caption)\n",
    "                caption_lengths.append(len(caption_text))\n",
    "                emoji_counts.append(emoji_count)\n",
    "                hashtag_counts.append(caption_text.count('#'))\n",
    "                mention_counts.append(caption_text.count('@'))\n",
    "        \n",
    "        # Compute features\n",
    "        features = {\n",
    "            'avg_likes': float(np.mean(likes)) if likes else 0.0,\n",
    "            'std_likes': float(np.std(likes)) if len(likes) > 1 else 0.0,\n",
    "            'avg_comments': float(np.mean(comments)) if comments else 0.0,\n",
    "            'avg_caption_length': float(np.mean(caption_lengths)) if caption_lengths else 0.0,\n",
    "            'avg_emoji_count': float(np.mean(emoji_counts)) if emoji_counts else 0.0,\n",
    "            'avg_hashtags': float(np.mean(hashtag_counts)) if hashtag_counts else 0.0,\n",
    "            'avg_mentions': float(np.mean(mention_counts)) if mention_counts else 0.0\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    def fit_transform(self, username2posts, username2profile):\n",
    "        \"\"\"Fit and transform the data\"\"\"\n",
    "        profile_features = []\n",
    "        post_features = []\n",
    "        post_texts = []\n",
    "        usernames = []\n",
    "        \n",
    "        for username, profile in username2profile.items():\n",
    "            usernames.append(username)\n",
    "            \n",
    "            # Extract profile features\n",
    "            prof_feats = self.extract_profile_features(profile)\n",
    "            profile_features.append(prof_feats)\n",
    "            \n",
    "            # Extract post features\n",
    "            posts = username2posts.get(username, [])\n",
    "            post_feats = self.extract_post_features(posts)\n",
    "            post_features.append(post_feats)\n",
    "            \n",
    "            # Aggregate post text\n",
    "            post_text = \"\\n\".join([\n",
    "                self.preprocess_text(post.get('caption', ''))[0]\n",
    "                for post in posts if post.get('caption')\n",
    "            ])\n",
    "            post_texts.append(post_text)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        profile_df = pd.DataFrame(profile_features, index=usernames)\n",
    "        post_df = pd.DataFrame(post_features, index=usernames)\n",
    "        \n",
    "        # TF-IDF for text\n",
    "        text_features = self.vectorizer.fit_transform(post_texts)\n",
    "        text_df = pd.DataFrame(\n",
    "            text_features.toarray(),\n",
    "            columns=self.vectorizer.get_feature_names_out(),\n",
    "            index=usernames\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined_df = pd.concat([profile_df, post_df, text_df], axis=1)\n",
    "        \n",
    "        # Scale numerical features\n",
    "        numerical_cols = profile_df.columns.tolist() + post_df.columns.tolist()\n",
    "        \n",
    "        # Log transform heavily skewed features\n",
    "        skewed_features = ['follower_count', 'following_count', 'post_count', \n",
    "                          'avg_likes', 'avg_comments']\n",
    "        for col in skewed_features:\n",
    "            if col in numerical_cols:\n",
    "                combined_df[col] = np.log1p(combined_df[col].clip(lower=0))\n",
    "        \n",
    "        # Handle following_ratio separately\n",
    "        if 'following_ratio' in numerical_cols:\n",
    "            ratio_99th = np.percentile(combined_df['following_ratio'], 99)\n",
    "            combined_df['following_ratio'] = combined_df['following_ratio'].clip(upper=ratio_99th)\n",
    "            combined_df['following_ratio'] = np.log1p(combined_df['following_ratio'].clip(lower=0))\n",
    "        \n",
    "        # Apply robust scaling\n",
    "        combined_df[numerical_cols] = self.scaler.fit_transform(combined_df[numerical_cols])\n",
    "        \n",
    "        return combined_df\n",
    "\n",
    "    def transform(self, username2posts, username2profile):\n",
    "        \"\"\"Transform new data using fitted parameters\"\"\"\n",
    "        profile_features = []\n",
    "        post_features = []\n",
    "        post_texts = []\n",
    "        usernames = []\n",
    "        \n",
    "        for username, profile in username2profile.items():\n",
    "            usernames.append(username)\n",
    "            \n",
    "            # Extract features\n",
    "            prof_feats = self.extract_profile_features(profile)\n",
    "            profile_features.append(prof_feats)\n",
    "            \n",
    "            posts = username2posts.get(username, [])\n",
    "            post_feats = self.extract_post_features(posts)\n",
    "            post_features.append(post_feats)\n",
    "            \n",
    "            post_text = \"\\n\".join([\n",
    "                self.preprocess_text(post.get('caption', ''))[0]\n",
    "                for post in posts if post.get('caption')\n",
    "            ])\n",
    "            post_texts.append(post_text)\n",
    "        \n",
    "        # Convert to DataFrames\n",
    "        profile_df = pd.DataFrame(profile_features, index=usernames)\n",
    "        post_df = pd.DataFrame(post_features, index=usernames)\n",
    "        \n",
    "        # Generate text features\n",
    "        text_features = self.vectorizer.transform(post_texts)\n",
    "        text_df = pd.DataFrame(\n",
    "            text_features.toarray(),\n",
    "            columns=self.vectorizer.get_feature_names_out(),\n",
    "            index=usernames\n",
    "        )\n",
    "        \n",
    "        # Combine features\n",
    "        combined_df = pd.concat([profile_df, post_df, text_df], axis=1)\n",
    "        \n",
    "        # Scale numerical features\n",
    "        numerical_cols = profile_df.columns.tolist() + post_df.columns.tolist()\n",
    "        \n",
    "        # Apply transformations\n",
    "        for col in ['follower_count', 'following_count', 'post_count', \n",
    "                   'avg_likes', 'avg_comments']:\n",
    "            if col in numerical_cols:\n",
    "                combined_df[col] = np.log1p(combined_df[col].clip(lower=0))\n",
    "        \n",
    "        if 'following_ratio' in numerical_cols:\n",
    "            ratio_99th = np.percentile(combined_df['following_ratio'], 99)\n",
    "            combined_df['following_ratio'] = combined_df['following_ratio'].clip(upper=ratio_99th)\n",
    "            combined_df['following_ratio'] = np.log1p(combined_df['following_ratio'].clip(lower=0))\n",
    "        \n",
    "        combined_df[numerical_cols] = self.scaler.transform(combined_df[numerical_cols])\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"Load and preprocess training and test data with combined annotations\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Load existing training classification labels\n",
    "    train_classification_df = pd.read_csv('train-classification.csv')\n",
    "    train_classification_df = train_classification_df.rename(\n",
    "        columns={'Unnamed: 0': 'user_id', 'label': 'category'}\n",
    "    )\n",
    "    train_classification_df[\"category\"] = train_classification_df[\"category\"].apply(str.lower)\n",
    "    \n",
    "    print(\"\\nBase training data distribution:\")\n",
    "    print(train_classification_df['category'].value_counts())\n",
    "    \n",
    "    # Load my annotations\n",
    "    annotated_df = pd.read_csv('annotated_users_CS412-411c414b075a.csv')\n",
    "    annotated_df['username'] = annotated_df['url'].apply(\n",
    "        lambda x: x.split('instagram.com/')[-1].split('&')[0]\n",
    "    )\n",
    "    annotated_df['category'] = annotated_df['influencerCategory'].str.lower()\n",
    "    \n",
    "    print(\"\\nAnnotated data distribution:\")\n",
    "    print(annotated_df['category'].value_counts())\n",
    "    \n",
    "    # Combine base training data with my annotated data\n",
    "    base_username2category = train_classification_df.set_index(\"user_id\")[\"category\"].to_dict()\n",
    "    annotated_username2category = annotated_df.set_index('username')['category'].to_dict()\n",
    "    \n",
    "    # Merge the dictionaries, prioritizing annotated data if there's overlap\n",
    "    username2category = {**base_username2category, **annotated_username2category}\n",
    "    \n",
    "    print(\"\\nCombined distribution:\")\n",
    "    combined_categories = pd.Series(username2category.values()).value_counts()\n",
    "    print(combined_categories)\n",
    "    print(f\"\\nTotal number of labeled profiles: {len(username2category)}\")\n",
    "\n",
    "    # Load and match profile data\n",
    "    username2posts_train = {}\n",
    "    username2profile_train = {}\n",
    "    processed_usernames = set()\n",
    "    \n",
    "    with gzip.open('training-dataset.jsonl.gz', \"rt\") as fh:\n",
    "        for line in fh:\n",
    "            sample = json.loads(line)\n",
    "            profile = sample[\"profile\"]\n",
    "            username = profile[\"username\"]\n",
    "            \n",
    "            if username in username2category and username not in processed_usernames:\n",
    "                username2posts_train[username] = sample[\"posts\"]\n",
    "                username2profile_train[username] = profile\n",
    "                processed_usernames.add(username)\n",
    "\n",
    "    print(f\"\\nMatched {len(username2posts_train)} profiles with posts and profile data\")\n",
    "    \n",
    "    final_categories = [username2category[username] for username in username2posts_train.keys()]\n",
    "    print(\"\\nFinal training data distribution:\")\n",
    "    print(pd.Series(final_categories).value_counts())\n",
    "\n",
    "    # Load test data\n",
    "    test_users = []\n",
    "    with open('test-classification-round3.dat', 'r') as f:\n",
    "        test_users = [line.strip() for line in f]\n",
    "    \n",
    "    test_posts = []\n",
    "    try:\n",
    "        with open('test-regression-round3.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    post = json.loads(line)\n",
    "                    test_posts.append(post)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test regression data: {str(e)}\")\n",
    "\n",
    "    # Load test profiles\n",
    "    username2posts_test = {}\n",
    "    username2profile_test = {}\n",
    "    \n",
    "    with gzip.open('training-dataset.jsonl.gz', \"rt\") as fh:\n",
    "        for line in fh:\n",
    "            sample = json.loads(line)\n",
    "            profile = sample[\"profile\"]\n",
    "            username = profile[\"username\"]\n",
    "            \n",
    "            if username in test_users:\n",
    "                username2posts_test[username] = sample[\"posts\"]\n",
    "                username2profile_test[username] = profile\n",
    "\n",
    "    print(f\"\\nTest data loaded: {len(username2posts_test)} profiles\")\n",
    "\n",
    "    return (username2posts_train, username2profile_train,\n",
    "            username2posts_test, username2profile_test,\n",
    "            username2category, test_posts)\n",
    "\n",
    "def prepare_regression_data(username2posts, username2profile):\n",
    "    \"\"\"Prepare the regression dataset\"\"\"\n",
    "    print(\"\\n=== PREPARING REGRESSION DATA ===\")\n",
    "    features = []\n",
    "    targets = []\n",
    "    \n",
    "    def safe_float(value, default=0.0):\n",
    "        if value is None:\n",
    "            return default\n",
    "        try:\n",
    "            return float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return default\n",
    "\n",
    "    def safe_len(value, default=0):\n",
    "        if value is None:\n",
    "            return default\n",
    "        try:\n",
    "            return len(str(value))\n",
    "        except (ValueError, TypeError):\n",
    "            return default\n",
    "\n",
    "    for username, posts in username2posts.items():\n",
    "        if not posts:  # Skip if no post\n",
    "            continue\n",
    "            \n",
    "        profile = username2profile.get(username, {})\n",
    "        \n",
    "        for post in posts:\n",
    "            if not isinstance(post, dict):  # Skip if post is not a dictionary\n",
    "                continue\n",
    "                \n",
    "            post_features = {\n",
    "                # Profile features\n",
    "                'follower_count': safe_float(profile.get('follower_count')),\n",
    "                'following_count': safe_float(profile.get('following_count')),\n",
    "                'post_count': safe_float(profile.get('post_count')),\n",
    "                'is_verified': float(profile.get('is_verified', False)),\n",
    "                'is_business': float(profile.get('is_business_account', False)),\n",
    "                \n",
    "                # Post features\n",
    "                'caption_length': safe_len(post.get('caption')),\n",
    "                'media_type_is_video': float(post.get('media_type') == 'VIDEO'),\n",
    "                'media_type_is_carousel': float(post.get('media_type') == 'CAROUSEL_ALBUM'),\n",
    "                'comments_count': safe_float(post.get('comments_count')),\n",
    "            }\n",
    "            \n",
    "            # Get target\n",
    "            like_count = safe_float(post.get('like_count'))\n",
    "            \n",
    "            if like_count > 0:  # Only include posts with valid like counts\n",
    "                features.append(list(post_features.values()))\n",
    "                targets.append(np.log10(like_count + 1))  # Log transform\n",
    "    \n",
    "    if not features:  # Check if we have any valid features\n",
    "        raise ValueError(\"No valid features could be extracted from the data\")\n",
    "    \n",
    "    X = np.array(features)\n",
    "    y = np.array(targets)\n",
    "    \n",
    "    print(f\"Regression dataset shape: X={X.shape}, y={y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "def train_classification_model(X_train, y_train):\n",
    "    \"\"\"Train the classification model using SMOTE for class balancing\"\"\"\n",
    "    print(\"\\n=== TRAINING CLASSIFICATION MODEL WITH SMOTE ===\")\n",
    "    \n",
    "    # Convert string labels to numerical using LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    \n",
    "    # Print original class distribution\n",
    "    print(\"\\nOriginal class distribution:\")\n",
    "    for label, count in zip(le.classes_, np.bincount(y_train_encoded)):\n",
    "        print(f\"{label}: {count}\")\n",
    "\n",
    "    # Apply SMOTE to balance the dataset\n",
    "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_encoded)\n",
    "    \n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    for label, count in zip(le.classes_, np.bincount(y_train_resampled)):\n",
    "        print(f\"{label}: {count}\")\n",
    "    \n",
    "    # Random Forest classifier\n",
    "    rf = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # parameters for RandomForest\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='balanced_accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Perform grid search on Smote data\n",
    "    print(\"\\nPerforming grid search with SMOTE-balanced data...\")\n",
    "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"\\nBest cross-validation balanced accuracy score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Print feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 most important features for classification:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Create a wrapped model that includes label encoding\n",
    "    class WrappedModel:\n",
    "        def __init__(self, model, label_encoder, smote):\n",
    "            self.model = model\n",
    "            self.label_encoder = label_encoder\n",
    "            self.smote = smote\n",
    "        \n",
    "        def predict(self, X):\n",
    "            y_pred_encoded = self.model.predict(X)\n",
    "            return self.label_encoder.inverse_transform(y_pred_encoded)\n",
    "        \n",
    "        @property\n",
    "        def feature_importances_(self):\n",
    "            return self.model.feature_importances_\n",
    "    \n",
    "    return WrappedModel(best_model, le, smote)\n",
    "def train_regression_model(X_train_reg, y_train_reg):\n",
    "    \"\"\"Train the regression model with hyperparameter tuning\"\"\"\n",
    "    print(\"\\n=== TRAINING REGRESSION MODEL ===\")\n",
    "    \n",
    "    # Define feature names\n",
    "    feature_names = [\n",
    "        'follower_count',\n",
    "        'following_count',\n",
    "        'post_count',\n",
    "        'is_verified',\n",
    "        'is_business',\n",
    "        'caption_length',\n",
    "        'media_type_is_video',\n",
    "        'media_type_is_carousel',\n",
    "        'comments_count'\n",
    "    ]\n",
    "    \n",
    "    # RandomForest Regressor parameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    # Initialize base regressor\n",
    "    base_reg = RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_reg,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # grid search\n",
    "    print(\"\\nPerforming grid search...\")\n",
    "    grid_search.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    print(\"\\nBest parameters found:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"\\nBest cross-validation R² score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Get best model\n",
    "    reg = grid_search.best_estimator_\n",
    "    \n",
    "    # Print feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names[:X_train_reg.shape[1]],\n",
    "        'importance': reg.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop features importance for regression:\")\n",
    "    print(feature_importance)\n",
    "    \n",
    "    return reg\n",
    "\n",
    "def extract_post_regression_features(post, profile):\n",
    "    \"\"\"Extract features for regression prediction of a single post\"\"\"\n",
    "    def safe_float(value, default=0.0):\n",
    "        if value is None:\n",
    "            return default\n",
    "        try:\n",
    "            return float(value)\n",
    "        except (ValueError, TypeError):\n",
    "            return default\n",
    "\n",
    "    def safe_len(value, default=0):\n",
    "        if value is None:\n",
    "            return default\n",
    "        try:\n",
    "            return len(str(value))\n",
    "        except (ValueError, TypeError):\n",
    "            return default\n",
    "\n",
    "    features = {\n",
    "        # Profile features\n",
    "        'follower_count': safe_float(profile.get('follower_count')),\n",
    "        'following_count': safe_float(profile.get('following_count')),\n",
    "        'post_count': safe_float(profile.get('post_count')),\n",
    "        'is_verified': float(profile.get('is_verified', False)),\n",
    "        'is_business': float(profile.get('is_business_account', False)),\n",
    "        \n",
    "        # Post features\n",
    "        'caption_length': safe_len(post.get('caption')),\n",
    "        'media_type_is_video': float(post.get('media_type') == 'VIDEO'),\n",
    "        'media_type_is_carousel': float(post.get('media_type') == 'CAROUSEL_ALBUM'),\n",
    "        'comments_count': safe_float(post.get('comments_count')),\n",
    "    }\n",
    "    \n",
    "    return list(features.values())\n",
    "\n",
    "def evaluate_models(clf, reg, X_val, y_val, X_val_reg, y_val_reg):\n",
    "    \"\"\"Evaluate both classification and regression models\"\"\"\n",
    "    print(\"\\n=== MODEL EVALUATION ===\")\n",
    "    \n",
    "    # evaluate classification\n",
    "    y_pred = clf.predict(X_val)\n",
    "    class_acc = accuracy_score(y_val, y_pred)\n",
    "    print(f\"\\nClassification Accuracy: {class_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred, zero_division=0))\n",
    "    \n",
    "    # evaluate reggresion\n",
    "    y_pred_reg = reg.predict(X_val_reg)\n",
    "    mse = mean_squared_error(y_val_reg, y_pred_reg)\n",
    "    mae = np.mean(np.abs(y_val_reg - y_pred_reg))\n",
    "    r2 = reg.score(X_val_reg, y_val_reg)\n",
    "    \n",
    "    print(f\"\\nRegression Metrics:\")\n",
    "    print(f\"MSE (log10): {mse:.4f}\") # this is what we look at\n",
    "    print(f\"MAE (log10): {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "def generate_predictions(clf, reg, feature_extractor, username2posts_test, username2profile_test, test_posts):\n",
    "    \"\"\"Generate predictions for test data\"\"\"\n",
    "    print(\"\\n=== GENERATING PREDICTIONS ===\")\n",
    "    \n",
    "    # classification predictions\n",
    "    print(\"\\nGenerating classification predictions...\")\n",
    "    X_test = feature_extractor.transform(username2posts_test, username2profile_test)\n",
    "    classification_predictions = {}\n",
    "    \n",
    "    for username in username2posts_test:\n",
    "        if username in X_test.index:\n",
    "            pred = clf.predict(X_test.loc[[username]])[0]\n",
    "            classification_predictions[username] = pred\n",
    "    \n",
    "    # regression predictions\n",
    "    print(\"\\nGenerating regression predictions...\")\n",
    "    regression_predictions = {}\n",
    "    \n",
    "    for post in test_posts:\n",
    "        post_id = str(post.get('id'))\n",
    "        if post_id:\n",
    "            username = post.get('username')\n",
    "            profile = username2profile_test.get(username, {})\n",
    "            \n",
    "            post_features = extract_post_regression_features(post, profile)\n",
    "            if post_features is not None:\n",
    "                pred_log = reg.predict([post_features])[0]\n",
    "                pred = int(np.power(10, pred_log))\n",
    "                regression_predictions[post_id] = pred\n",
    "    \n",
    "    return classification_predictions, regression_predictions\n",
    "def save_predictions(predictions, filename):\n",
    "    \"\"\"Save predictions to JSON file\"\"\"\n",
    "    print(f\"\\nSaving predictions to {filename}\")\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Successfully saved predictions to {filename}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Starting Instagram Influencer Classification and Regression Task...\")\n",
    "    \n",
    "    # load all data\n",
    "    (username2posts_train, username2profile_train,\n",
    "     username2posts_test, username2profile_test,\n",
    "     username2category, test_posts) = load_data()\n",
    "    \n",
    "    feature_extractor = InfluencerFeatureExtractor()\n",
    "    \n",
    "    # prepare training data for classification\n",
    "    print(\"\\nPreparing classification training data...\")\n",
    "    X = feature_extractor.fit_transform(username2posts_train, username2profile_train)\n",
    "    y = [username2category[username] for username in X.index]\n",
    "    \n",
    "    # Split for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # train classification model\n",
    "    clf = train_classification_model(X_train, y_train)\n",
    "    \n",
    "    # same process for reg\n",
    "    print(\"\\nPreparing regression data...\")\n",
    "    X_train_reg, y_train_reg = prepare_regression_data(\n",
    "        username2posts_train, username2profile_train\n",
    "    )\n",
    "    X_train_reg, X_val_reg, y_train_reg, y_val_reg = train_test_split(\n",
    "        X_train_reg, y_train_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    reg = train_regression_model(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # evaluate models\n",
    "    evaluate_models(clf, reg, X_val, y_val, X_val_reg, y_val_reg)\n",
    "    \n",
    "    # generate predictions for round test data\n",
    "    class_predictions, reg_predictions = generate_predictions(\n",
    "        clf, reg, feature_extractor,\n",
    "        username2posts_test, username2profile_test,\n",
    "        test_posts\n",
    "    )\n",
    "    # save it\n",
    "    save_predictions(class_predictions, 'prediction-classification-round3.json')\n",
    "    save_predictions(reg_predictions, 'prediction-regression-round3.json')\n",
    "    \n",
    "    print(\"\\nTask completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
